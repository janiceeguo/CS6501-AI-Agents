{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3"
      ],
      "metadata": {
        "id": "YHuqKQXx8tgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpXpMtTf9lCQ",
        "outputId": "74dc420e-c9eb-464f-c139-67e5b3f17483"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-1.1.8-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.9 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.2.9)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (2.17.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.9->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.9->langchain_openai) (0.6.9)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.9->langchain_openai) (26.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.9->langchain_openai) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.9->langchain_openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.9->langchain_openai) (9.1.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.9->langchain_openai) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.9->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.13.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.67.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain_openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.9->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.9->langchain_openai) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.9->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.9->langchain_openai) (3.6.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.9->langchain_openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.9->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.9->langchain_openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.9->langchain_openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.5.0)\n",
            "Downloading langchain_openai-1.1.8-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_openai\n",
            "Successfully installed langchain_openai-1.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "# Make it available like system env vars\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "JzcTt42EBAn5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrJXdy-p8nKt",
        "outputId": "049af7aa-f1f1-41c4-dfcf-1f2c95031585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "LangGraph Manual Tool Calling - Persistent Multi-Turn Conversation\n",
            "================================================================================\n",
            "\n",
            "This system uses manual tool calling with ToolNode:\n",
            "  - call_model: Invokes LLM with tool bindings\n",
            "  - ToolNode: Executes requested tools in parallel\n",
            "  - Loop: tools -> call_model (until no more tools needed)\n",
            "  - Single persistent conversation across all turns\n",
            "  - History managed automatically (trimmed after 100 messages)\n",
            "\n",
            "Commands:\n",
            "  - Type 'quit' or 'exit' to end the conversation\n",
            "  - Type 'verbose' to enable detailed tracing\n",
            "  - Type 'quiet' to disable detailed tracing\n",
            "\n",
            "Available tools:\n",
            "  - get_weather(location): Get weather information\n",
            "  - get_population(city): Get population data\n",
            "  - calculate(expression): Evaluate math expressions\n",
            "================================================================================\n",
            "[SYSTEM] Conversation graph created successfully (using manual ToolNode)\n",
            "[SYSTEM] Graph visualization saved to 'langchain_manual_tool_graph.png'\n",
            "\n",
            "[SYSTEM] Starting conversation...\n",
            "\n",
            "\n",
            "================================================================================\n",
            "NODE: input_node\n",
            "================================================================================\n",
            "\n",
            "You: what is the weather in charlottesville\n",
            "[DEBUG] User input: what is the weather in charlottesville\n",
            "[DEBUG] Routing to call_model\n",
            "\n",
            "================================================================================\n",
            "NODE: call_model\n",
            "================================================================================\n",
            "[DEBUG] Calling model with 1 messages\n",
            "[DEBUG] Added system prompt\n",
            "[DEBUG] Model requested 1 tool call(s):\n",
            "  - get_weather({'location': 'charlottesville'})\n",
            "[DEBUG] Routing to tools\n",
            "\n",
            "================================================================================\n",
            "NODE: call_model\n",
            "================================================================================\n",
            "[DEBUG] Calling model with 4 messages\n",
            "[DEBUG] Added system prompt\n",
            "[DEBUG] Model response (no tools): The current weather in Charlottesville is sunny with a temperature of 72°F and light winds....\n",
            "[DEBUG] Routing to output\n",
            "\n",
            "================================================================================\n",
            "NODE: output_node\n",
            "================================================================================\n",
            "\n",
            "Assistant: The current weather in Charlottesville is sunny with a temperature of 72°F and light winds.\n",
            "\n",
            "================================================================================\n",
            "NODE: input_node\n",
            "================================================================================\n",
            "\n",
            "You: what is the population of charlottesville\n",
            "[DEBUG] User input: what is the population of charlottesville\n",
            "[DEBUG] Routing to call_model\n",
            "\n",
            "================================================================================\n",
            "NODE: call_model\n",
            "================================================================================\n",
            "[DEBUG] Calling model with 7 messages\n",
            "[DEBUG] Added system prompt\n",
            "[DEBUG] Model requested 1 tool call(s):\n",
            "  - get_population({'city': 'charlottesville'})\n",
            "[DEBUG] Routing to tools\n",
            "\n",
            "================================================================================\n",
            "NODE: call_model\n",
            "================================================================================\n",
            "[DEBUG] Calling model with 10 messages\n",
            "[DEBUG] Added system prompt\n",
            "[DEBUG] Model response (no tools): Charlottesville has a population of approximately 1 million people....\n",
            "[DEBUG] Routing to output\n",
            "\n",
            "================================================================================\n",
            "NODE: output_node\n",
            "================================================================================\n",
            "\n",
            "Assistant: Charlottesville has a population of approximately 1 million people.\n",
            "\n",
            "================================================================================\n",
            "NODE: input_node\n",
            "================================================================================\n",
            "\n",
            "You: exit\n",
            "[DEBUG] Exit command received\n",
            "[DEBUG] Routing to END (exit requested)\n",
            "\n",
            "[SYSTEM] Conversation ended. Goodbye!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "LangGraph Multi-Agent System with Manual ToolNode Implementation\n",
        "\n",
        "This program demonstrates a LangGraph application with manual tool calling:\n",
        "- A single persistent conversation across multiple turns\n",
        "- Manual tool calling loop with ToolNode (no create_react_agent)\n",
        "- Graph-based looping (no Python loops or checkpointing)\n",
        "- Automatic conversation history management (trimming after 100 messages)\n",
        "- Verbose debugging output\n",
        "\"\"\"\n",
        "\n",
        "import asyncio\n",
        "from typing import TypedDict, Annotated, Sequence, Literal\n",
        "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# ============================================================================\n",
        "# STATE DEFINITION\n",
        "# ============================================================================\n",
        "\n",
        "class ConversationState(TypedDict):\n",
        "    \"\"\"\n",
        "    State schema for the conversation.\n",
        "\n",
        "    Attributes:\n",
        "        messages: Full conversation history with automatic message merging\n",
        "        verbose: Controls detailed tracing output\n",
        "        command: Special command from user (exit, verbose, quiet, or None)\n",
        "    \"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    verbose: bool\n",
        "    command: str  # \"exit\", \"verbose\", \"quiet\", or None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TOOL DEFINITIONS\n",
        "# ============================================================================\n",
        "\n",
        "@tool\n",
        "async def get_weather(location: str) -> str:\n",
        "    \"\"\"\n",
        "    Get current weather information for a specified location.\n",
        "\n",
        "    Args:\n",
        "        location: City name or location string\n",
        "\n",
        "    Returns:\n",
        "        Weather description string\n",
        "    \"\"\"\n",
        "    # Simulate API call delay\n",
        "    await asyncio.sleep(0.5)\n",
        "    return f\"Weather in {location}: Sunny, 72Â°F with light winds\"\n",
        "\n",
        "\n",
        "@tool\n",
        "async def get_population(city: str) -> str:\n",
        "    \"\"\"\n",
        "    Get population information for a specified city.\n",
        "\n",
        "    Args:\n",
        "        city: City name\n",
        "\n",
        "    Returns:\n",
        "        Population information string\n",
        "    \"\"\"\n",
        "    # Simulate API call delay\n",
        "    await asyncio.sleep(0.5)\n",
        "    return f\"Population of {city}: Approximately 1 million people\"\n",
        "\n",
        "\n",
        "@tool\n",
        "async def calculate(expression: str) -> str:\n",
        "    \"\"\"\n",
        "    Evaluate a mathematical expression.\n",
        "\n",
        "    Args:\n",
        "        expression: Mathematical expression to evaluate (e.g., \"2 + 2\")\n",
        "\n",
        "    Returns:\n",
        "        Result of the calculation\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Safe evaluation of simple math expressions\n",
        "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
        "        return f\"Result: {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error calculating: {str(e)}\"\n",
        "\n",
        "\n",
        "# List of all available tools\n",
        "tools = [get_weather, get_population, calculate]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# NODE FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def input_node(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Get input from the user and add it to the conversation.\n",
        "\n",
        "    This node:\n",
        "    - Prompts the user for input\n",
        "    - Handles special commands (quit, exit, verbose, quiet)\n",
        "    - Adds user message to conversation history (for real messages only)\n",
        "    - Sets command field for special commands\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with new user message or command\n",
        "    \"\"\"\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"NODE: input_node\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    # Get user input\n",
        "    user_input = input(\"\\nYou: \").strip()\n",
        "\n",
        "    # Handle exit commands\n",
        "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Exit command received\")\n",
        "        # Set command field, don't add to messages\n",
        "        return {\"command\": \"exit\"}\n",
        "\n",
        "    # Handle verbose toggle\n",
        "    if user_input.lower() == \"verbose\":\n",
        "        print(\"[SYSTEM] Verbose mode enabled\")\n",
        "        # Set command field and update verbose flag\n",
        "        return {\"command\": \"verbose\", \"verbose\": True}\n",
        "\n",
        "    if user_input.lower() == \"quiet\":\n",
        "        print(\"[SYSTEM] Verbose mode disabled\")\n",
        "        # Set command field and update verbose flag\n",
        "        return {\"command\": \"quiet\", \"verbose\": False}\n",
        "\n",
        "    # Add user message to conversation history\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(f\"[DEBUG] User input: {user_input}\")\n",
        "\n",
        "    # Clear command field and add message\n",
        "    return {\"command\": None, \"messages\": [HumanMessage(content=user_input)]}\n",
        "\n",
        "\n",
        "def call_model(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Call the LLM with tools bound.\n",
        "\n",
        "    This node:\n",
        "    - Prepends system message if not already present\n",
        "    - Invokes the model with tool bindings\n",
        "    - Returns the model's response (may include tool_calls)\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with model response\n",
        "    \"\"\"\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"NODE: call_model\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"[DEBUG] Calling model with {len(state['messages'])} messages\")\n",
        "\n",
        "    messages = list(state[\"messages\"])\n",
        "\n",
        "    # Add system message if not present\n",
        "    # Check if first message is a SystemMessage\n",
        "    system_added = False\n",
        "    if not messages or not isinstance(messages[0], SystemMessage):\n",
        "        system_prompt = SystemMessage(\n",
        "            content=\"You are a helpful assistant. \"\n",
        "                   \"If a tool is able to solve a problem you are working on then \"\n",
        "                   \"always use it, even if you are able to solve it without using a tool.\"\n",
        "        )\n",
        "        messages = [system_prompt] + messages\n",
        "        system_added = True\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Added system prompt\")\n",
        "\n",
        "    # Initialize model with tools\n",
        "    model = ChatOpenAI(\n",
        "        model=\"gpt-4o\",\n",
        "        temperature=0.7\n",
        "    )\n",
        "    model_with_tools = model.bind_tools(tools)\n",
        "\n",
        "    # Invoke the model\n",
        "    response = model_with_tools.invoke(messages)\n",
        "\n",
        "    if state.get(\"verbose\", True):\n",
        "        if hasattr(response, 'tool_calls') and response.tool_calls:\n",
        "            print(f\"[DEBUG] Model requested {len(response.tool_calls)} tool call(s):\")\n",
        "            for tc in response.tool_calls:\n",
        "                print(f\"  - {tc['name']}({tc['args']})\")\n",
        "        else:\n",
        "            print(f\"[DEBUG] Model response (no tools): {response.content[:100]}...\")\n",
        "\n",
        "    # Return the system message if we added it, plus the response\n",
        "    # This ensures the system message is persisted in the conversation state\n",
        "    if system_added:\n",
        "        return {\"messages\": [messages[0], response]}  # system prompt + model response\n",
        "    else:\n",
        "        return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "def output_node(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Display the assistant's final response to the user.\n",
        "\n",
        "    This node:\n",
        "    - Extracts the last AI message from the conversation\n",
        "    - Prints it to the console\n",
        "    - Returns empty dict (no state changes)\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Empty dict (no state modifications)\n",
        "    \"\"\"\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"NODE: output_node\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    # Find the last AI message in the conversation\n",
        "    # (there may be tool messages mixed in)\n",
        "    last_ai_message = None\n",
        "    for msg in reversed(state[\"messages\"]):\n",
        "        if isinstance(msg, AIMessage) and msg.content:\n",
        "            last_ai_message = msg\n",
        "            break\n",
        "\n",
        "    if last_ai_message:\n",
        "        print(f\"\\nAssistant: {last_ai_message.content}\")\n",
        "    else:\n",
        "        print(\"\\n[WARNING] No assistant response found\")\n",
        "\n",
        "    return {}\n",
        "\n",
        "\n",
        "def trim_history(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Manage conversation history length to prevent unlimited growth.\n",
        "\n",
        "    Strategy:\n",
        "    - Keep the system message (if present)\n",
        "    - Keep the most recent 100 messages\n",
        "    - This allows ~50 conversation turns (user + assistant pairs)\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with trimmed message history (if needed)\n",
        "    \"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    max_messages = 100\n",
        "\n",
        "    # Only trim if we've exceeded the limit\n",
        "    if len(messages) > max_messages:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(f\"\\n[DEBUG] History length: {len(messages)} messages\")\n",
        "            print(f\"[DEBUG] Trimming to most recent {max_messages} messages\")\n",
        "\n",
        "        # Preserve system message if it exists at the start\n",
        "        if messages and isinstance(messages[0], SystemMessage):\n",
        "            # Keep system message + last (max_messages - 1) messages\n",
        "            trimmed = [messages[0]] + list(messages[-(max_messages - 1):])\n",
        "            if state.get(\"verbose\", True):\n",
        "                print(f\"[DEBUG] Preserved system message + {max_messages - 1} recent messages\")\n",
        "        else:\n",
        "            # Just keep the last max_messages\n",
        "            trimmed = list(messages[-max_messages:])\n",
        "            if state.get(\"verbose\", True):\n",
        "                print(f\"[DEBUG] Kept {max_messages} most recent messages\")\n",
        "\n",
        "        return {\"messages\": trimmed}\n",
        "\n",
        "    # No trimming needed\n",
        "    return {}\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ROUTING LOGIC\n",
        "# ============================================================================\n",
        "\n",
        "def route_after_input(state: ConversationState) -> Literal[\"call_model\", \"end\", \"input\"]:\n",
        "    \"\"\"\n",
        "    Determine where to route after input based on command field.\n",
        "\n",
        "    Logic:\n",
        "    - If command is \"exit\", route to END\n",
        "    - If command is \"verbose\" or \"quiet\", route back to input\n",
        "    - Otherwise (command is None), route to call_model\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        \"end\" to terminate, \"input\" for verbose toggle, \"call_model\" to continue\n",
        "    \"\"\"\n",
        "    command = state.get(\"command\")\n",
        "\n",
        "    # Check for exit command\n",
        "    if command == \"exit\":\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Routing to END (exit requested)\")\n",
        "        return \"end\"\n",
        "\n",
        "    # Check for verbose toggle commands - route back to input\n",
        "    if command in [\"verbose\", \"quiet\"]:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Routing back to input (verbose toggle)\")\n",
        "        return \"input\"\n",
        "\n",
        "    # Normal message - route to model\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"[DEBUG] Routing to call_model\")\n",
        "    return \"call_model\"\n",
        "\n",
        "\n",
        "def route_after_model(state: ConversationState) -> Literal[\"tools\", \"output\"]:\n",
        "    \"\"\"\n",
        "    Route after model call based on whether tools were requested.\n",
        "\n",
        "    Logic:\n",
        "    - If the model's response includes tool_calls, route to tools\n",
        "    - Otherwise, route to output to display the response\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        \"tools\" if tools requested, \"output\" otherwise\n",
        "    \"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "\n",
        "    # Check if the last message has tool calls\n",
        "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Routing to tools\")\n",
        "        return \"tools\"\n",
        "\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"[DEBUG] Routing to output\")\n",
        "    return \"output\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# GRAPH CONSTRUCTION\n",
        "# ============================================================================\n",
        "\n",
        "def create_conversation_graph():\n",
        "    \"\"\"\n",
        "    Build the conversation graph with manual tool calling using ToolNode.\n",
        "\n",
        "    Graph structure (single conversation with looping):\n",
        "\n",
        "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "        â”‚                                                      â”‚\n",
        "        â–¼                                                      â”‚\n",
        "      input_node â”€â”€(check command)â”€â”€> call_model              â”‚\n",
        "          â–²                              â”‚                     â”‚\n",
        "          â”‚                              â”œâ”€â”€(has tools)â”€â”€> tools\n",
        "          â”‚                              â”‚                    â”‚\n",
        "          â”‚                              â”‚                    â”‚\n",
        "          â”‚                              â””â”€â”€(no tools)â”€â”€> output_node\n",
        "          â”‚                                                    â”‚\n",
        "          â”‚                                                    â–¼\n",
        "          â””â”€â”€â”€(verbose/quiet)                           trim_history â”€â”€â”˜\n",
        "\n",
        "          â””â”€â”€â”€â”€â”€(exit)â”€â”€> END\n",
        "\n",
        "    Key features:\n",
        "    - Manual tool calling with ToolNode (no create_react_agent)\n",
        "    - Command field used for special commands (no sentinel messages!)\n",
        "    - Single conversation maintained in state.messages\n",
        "    - Graph loops back to input_node after each turn\n",
        "    - Tools route back to call_model for continued reasoning\n",
        "    - History automatically trimmed when it grows too long\n",
        "\n",
        "    Returns:\n",
        "        Compiled LangGraph application\n",
        "    \"\"\"\n",
        "\n",
        "    # ========================================================================\n",
        "    # Create the Conversation Graph\n",
        "    # ========================================================================\n",
        "\n",
        "    workflow = StateGraph(ConversationState)\n",
        "\n",
        "    # Create ToolNode to handle tool execution\n",
        "    # ToolNode automatically executes tools in parallel when possible\n",
        "    tool_node = ToolNode(tools)\n",
        "\n",
        "    # Add all nodes\n",
        "    workflow.add_node(\"input\", input_node)\n",
        "    workflow.add_node(\"call_model\", call_model)\n",
        "    workflow.add_node(\"tools\", tool_node)  # ToolNode handles tool execution\n",
        "    workflow.add_node(\"output\", output_node)\n",
        "    workflow.add_node(\"trim_history\", trim_history)\n",
        "\n",
        "    # Set entry point - conversation always starts at input\n",
        "    workflow.set_entry_point(\"input\")\n",
        "\n",
        "    # Add conditional edge from input based on command field\n",
        "    # Check if user wants to exit, toggle verbose, or continue\n",
        "    workflow.add_conditional_edges(\n",
        "        \"input\",\n",
        "        route_after_input,\n",
        "        {\n",
        "            \"call_model\": \"call_model\",\n",
        "            \"input\": \"input\",  # Loop back for verbose/quiet\n",
        "            \"end\": END\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Add conditional edge from call_model\n",
        "    # Check if tools were requested or if we have final response\n",
        "    workflow.add_conditional_edges(\n",
        "        \"call_model\",\n",
        "        route_after_model,\n",
        "        {\n",
        "            \"tools\": \"tools\",\n",
        "            \"output\": \"output\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # After tools execute, loop back to call_model\n",
        "    # This allows the model to see tool results and decide next action\n",
        "    workflow.add_edge(\"tools\", \"call_model\")\n",
        "\n",
        "    # After output, trim history and loop back to input\n",
        "    workflow.add_edge(\"output\", \"trim_history\")\n",
        "    workflow.add_edge(\"trim_history\", \"input\")  # This creates the conversation loop!\n",
        "\n",
        "    # Compile the graph\n",
        "    print(\"[SYSTEM] Conversation graph created successfully (using manual ToolNode)\")\n",
        "    return workflow.compile()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def visualize_graph(app):\n",
        "    \"\"\"\n",
        "    Generate Mermaid diagram of the conversation graph.\n",
        "\n",
        "    Creates:\n",
        "    - langchain_manual_tool_graph.png: The conversation loop with manual tool calling\n",
        "\n",
        "    Args:\n",
        "        app: Compiled conversation graph\n",
        "    \"\"\"\n",
        "    try:\n",
        "        graph_png = app.get_graph().draw_mermaid_png()\n",
        "        with open(\"langchain_manual_tool_graph.png\", \"wb\") as f:\n",
        "            f.write(graph_png)\n",
        "        print(\"[SYSTEM] Graph visualization saved to 'langchain_manual_tool_graph.png'\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARNING] Could not generate graph visualization: {e}\")\n",
        "        print(\"You may need to install: pip install pygraphviz or pip install grandalf\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "async def main():\n",
        "    \"\"\"\n",
        "    Main execution function.\n",
        "\n",
        "    This function:\n",
        "    1. Creates the conversation graph\n",
        "    2. Visualizes the graph structure\n",
        "    3. Initializes the conversation state\n",
        "    4. Invokes the graph ONCE\n",
        "\n",
        "    The graph then runs indefinitely via internal looping (trim_history -> input)\n",
        "    until the user types 'quit' or 'exit'.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"LangGraph Manual Tool Calling - Persistent Multi-Turn Conversation\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nThis system uses manual tool calling with ToolNode:\")\n",
        "    print(\"  - call_model: Invokes LLM with tool bindings\")\n",
        "    print(\"  - ToolNode: Executes requested tools in parallel\")\n",
        "    print(\"  - Loop: tools -> call_model (until no more tools needed)\")\n",
        "    print(\"  - Single persistent conversation across all turns\")\n",
        "    print(\"  - History managed automatically (trimmed after 100 messages)\")\n",
        "    print(\"\\nCommands:\")\n",
        "    print(\"  - Type 'quit' or 'exit' to end the conversation\")\n",
        "    print(\"  - Type 'verbose' to enable detailed tracing\")\n",
        "    print(\"  - Type 'quiet' to disable detailed tracing\")\n",
        "    print(\"\\nAvailable tools:\")\n",
        "    print(\"  - get_weather(location): Get weather information\")\n",
        "    print(\"  - get_population(city): Get population data\")\n",
        "    print(\"  - calculate(expression): Evaluate math expressions\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create the conversation graph\n",
        "    app = create_conversation_graph()\n",
        "\n",
        "    # Visualize the graph\n",
        "    visualize_graph(app)\n",
        "\n",
        "    # Initialize conversation state\n",
        "    # This state persists across all turns via graph looping\n",
        "    initial_state = {\n",
        "        \"messages\": [],\n",
        "        \"verbose\": True,\n",
        "        \"command\": None\n",
        "    }\n",
        "\n",
        "    print(\"\\n[SYSTEM] Starting conversation...\\n\")\n",
        "\n",
        "    try:\n",
        "        # Invoke the graph ONCE\n",
        "        # The graph will loop internally until user exits\n",
        "        # Each iteration: input -> call_model -> [tools -> call_model]* -> output -> trim -> input\n",
        "        # Verbose commands: input -> input (direct loop!)\n",
        "        await app.ainvoke(initial_state)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\n[SYSTEM] Interrupted by user (Ctrl+C)\")\n",
        "\n",
        "    print(\"\\n[SYSTEM] Conversation ended. Goodbye!\\n\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ENTRY POINT\n",
        "# ============================================================================\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # asyncio.run() executes main() exactly ONCE\n",
        "#     # The looping happens INSIDE the graph via edges\n",
        "#     asyncio.run(main())\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        asyncio.run(main())\n",
        "    except RuntimeError:\n",
        "        # Running inside Jupyter\n",
        "        import nest_asyncio\n",
        "        nest_asyncio.apply()\n",
        "        asyncio.run(main())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "LangGraph ReAct Agent with Persistent Multi-Turn Conversation\n",
        "\n",
        "This program demonstrates a LangGraph application using create_react_agent with:\n",
        "- A single persistent conversation across multiple turns\n",
        "- Graph-based looping (no Python loops or checkpointing)\n",
        "- Automatic conversation history management (trimming after 100 messages)\n",
        "- Verbose debugging output\n",
        "\"\"\"\n",
        "\n",
        "import asyncio\n",
        "import time\n",
        "from typing import TypedDict, Annotated, Sequence, Literal\n",
        "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# ============================================================================\n",
        "# STATE DEFINITION\n",
        "# ============================================================================\n",
        "\n",
        "class ConversationState(TypedDict):\n",
        "    \"\"\"\n",
        "    State schema for the conversation.\n",
        "\n",
        "    Attributes:\n",
        "        messages: Full conversation history with automatic message merging\n",
        "        verbose: Controls detailed tracing output\n",
        "        command: Special command from user (exit, verbose, quiet, or None)\n",
        "    \"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    verbose: bool\n",
        "    command: str  # \"exit\", \"verbose\", \"quiet\", or None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TOOL DEFINITIONS\n",
        "# ============================================================================\n",
        "\n",
        "@tool\n",
        "def get_weather(location: str) -> str:\n",
        "    \"\"\"\n",
        "    Get current weather information for a specified location.\n",
        "\n",
        "    Args:\n",
        "        location: City name or location string\n",
        "\n",
        "    Returns:\n",
        "        Weather description string\n",
        "    \"\"\"\n",
        "    # Simulate API call delay\n",
        "    time.sleep(0.5)\n",
        "    return f\"Weather in {location}: Sunny, 72Â°F with light winds\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_population(city: str) -> str:\n",
        "    \"\"\"\n",
        "    Get population information for a specified city.\n",
        "\n",
        "    Args:\n",
        "        city: City name\n",
        "\n",
        "    Returns:\n",
        "        Population information string\n",
        "    \"\"\"\n",
        "    # Simulate API call delay\n",
        "    time.sleep(0.5)\n",
        "    return f\"Population of {city}: Approximately 1 million people\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def calculate(expression: str) -> str:\n",
        "    \"\"\"\n",
        "    Evaluate a mathematical expression.\n",
        "\n",
        "    Args:\n",
        "        expression: Mathematical expression to evaluate (e.g., \"2 + 2\")\n",
        "\n",
        "    Returns:\n",
        "        Result of the calculation\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Safe evaluation of simple math expressions\n",
        "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
        "        return f\"Result: {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error calculating: {str(e)}\"\n",
        "\n",
        "\n",
        "# List of all available tools\n",
        "tools = [get_weather, get_population, calculate]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# NODE FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def input_node(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Get input from the user and add it to the conversation.\n",
        "\n",
        "    This node:\n",
        "    - Prompts the user for input\n",
        "    - Handles special commands (quit, exit, verbose, quiet)\n",
        "    - Adds user message to conversation history (for real messages only)\n",
        "    - Sets command field for special commands\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with new user message or command\n",
        "    \"\"\"\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"NODE: input_node\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    # Get user input\n",
        "    user_input = input(\"\\nYou: \").strip()\n",
        "\n",
        "    # Handle exit commands\n",
        "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Exit command received\")\n",
        "        # Set command field, don't add to messages\n",
        "        return {\"command\": \"exit\"}\n",
        "\n",
        "    # Handle verbose toggle\n",
        "    if user_input.lower() == \"verbose\":\n",
        "        print(\"[SYSTEM] Verbose mode enabled\")\n",
        "        # Set command field and update verbose flag\n",
        "        return {\"command\": \"verbose\", \"verbose\": True}\n",
        "\n",
        "    if user_input.lower() == \"quiet\":\n",
        "        print(\"[SYSTEM] Verbose mode disabled\")\n",
        "        # Set command field and update verbose flag\n",
        "        return {\"command\": \"quiet\", \"verbose\": False}\n",
        "\n",
        "    # Add user message to conversation history\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(f\"[DEBUG] User input: {user_input}\")\n",
        "\n",
        "    # Clear command field and add message\n",
        "    return {\"command\": None, \"messages\": [HumanMessage(content=user_input)]}\n",
        "\n",
        "\n",
        "def call_react_agent(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Invoke the ReAct agent with the current conversation history.\n",
        "\n",
        "    This node:\n",
        "    - Takes the full conversation history from state\n",
        "    - Invokes the ReAct agent (which handles tool calling internally)\n",
        "    - Returns only the NEW messages generated by the agent\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with agent's response messages\n",
        "    \"\"\"\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"NODE: call_react_agent\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"[DEBUG] Invoking ReAct agent with {len(state['messages'])} messages in history\")\n",
        "\n",
        "    # Get the global react_agent\n",
        "    global react_agent\n",
        "\n",
        "    # Count messages before agent call\n",
        "    messages_before = len(state[\"messages\"])\n",
        "\n",
        "    # Invoke the ReAct agent with full conversation history\n",
        "    # The agent maintains context across all previous turns\n",
        "    result = react_agent.invoke({\"messages\": state[\"messages\"]})\n",
        "\n",
        "    if state.get(\"verbose\", True):\n",
        "        messages_after = len(result[\"messages\"])\n",
        "        new_message_count = messages_after - messages_before\n",
        "        print(f\"[DEBUG] Agent generated {new_message_count} new messages\")\n",
        "\n",
        "        # Show what the agent did\n",
        "        for msg in result[\"messages\"][messages_before:]:\n",
        "            if isinstance(msg, AIMessage):\n",
        "                if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "                    print(f\"[DEBUG] Tool calls: {[tc['name'] for tc in msg.tool_calls]}\")\n",
        "                elif msg.content:\n",
        "                    print(f\"[DEBUG] Response preview: {msg.content[:100]}...\")\n",
        "\n",
        "    # Return only the NEW messages (everything after what we sent)\n",
        "    new_messages = result[\"messages\"][messages_before:]\n",
        "    return {\"messages\": new_messages}\n",
        "\n",
        "\n",
        "def output_node(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Display the assistant's final response to the user.\n",
        "\n",
        "    This node:\n",
        "    - Extracts the last AI message from the conversation\n",
        "    - Prints it to the console\n",
        "    - Returns empty dict (no state changes)\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Empty dict (no state modifications)\n",
        "    \"\"\"\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"NODE: output_node\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    # Find the last AI message in the conversation\n",
        "    # (there may be tool messages mixed in)\n",
        "    last_ai_message = None\n",
        "    for msg in reversed(state[\"messages\"]):\n",
        "        if isinstance(msg, AIMessage) and msg.content:\n",
        "            last_ai_message = msg\n",
        "            break\n",
        "\n",
        "    if last_ai_message:\n",
        "        print(f\"\\nAssistant: {last_ai_message.content}\")\n",
        "    else:\n",
        "        print(\"\\n[WARNING] No assistant response found\")\n",
        "\n",
        "    return {}\n",
        "\n",
        "\n",
        "def trim_history(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Manage conversation history length to prevent unlimited growth.\n",
        "\n",
        "    Strategy:\n",
        "    - Keep the system message (if present)\n",
        "    - Keep the most recent 100 messages\n",
        "    - This allows ~50 conversation turns (user + assistant pairs)\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with trimmed message history (if needed)\n",
        "    \"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    max_messages = 100\n",
        "\n",
        "    # Only trim if we've exceeded the limit\n",
        "    if len(messages) > max_messages:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(f\"\\n[DEBUG] History length: {len(messages)} messages\")\n",
        "            print(f\"[DEBUG] Trimming to most recent {max_messages} messages\")\n",
        "\n",
        "        # Preserve system message if it exists at the start\n",
        "        if messages and isinstance(messages[0], SystemMessage):\n",
        "            # Keep system message + last (max_messages - 1) messages\n",
        "            trimmed = [messages[0]] + list(messages[-(max_messages - 1):])\n",
        "            if state.get(\"verbose\", True):\n",
        "                print(f\"[DEBUG] Preserved system message + {max_messages - 1} recent messages\")\n",
        "        else:\n",
        "            # Just keep the last max_messages\n",
        "            trimmed = list(messages[-max_messages:])\n",
        "            if state.get(\"verbose\", True):\n",
        "                print(f\"[DEBUG] Kept {max_messages} most recent messages\")\n",
        "\n",
        "        return {\"messages\": trimmed}\n",
        "\n",
        "    # No trimming needed\n",
        "    return {}\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ROUTING LOGIC\n",
        "# ============================================================================\n",
        "\n",
        "def route_after_input(state: ConversationState) -> Literal[\"call_react_agent\", \"end\", \"input\"]:\n",
        "    \"\"\"\n",
        "    Determine where to route after input based on command field.\n",
        "\n",
        "    Logic:\n",
        "    - If command is \"exit\", route to END\n",
        "    - If command is \"verbose\" or \"quiet\", route back to input\n",
        "    - Otherwise (command is None), route to the ReAct agent\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        \"end\" to terminate, \"input\" for verbose toggle, \"call_react_agent\" to continue\n",
        "    \"\"\"\n",
        "    command = state.get(\"command\")\n",
        "\n",
        "    # Check for exit command\n",
        "    if command == \"exit\":\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Routing to END (exit requested)\")\n",
        "        return \"end\"\n",
        "\n",
        "    # Check for verbose toggle commands - route back to input\n",
        "    if command in [\"verbose\", \"quiet\"]:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Routing back to input (verbose toggle)\")\n",
        "        return \"input\"\n",
        "\n",
        "    # Normal message - route to agent\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"[DEBUG] Routing to call_react_agent\")\n",
        "    return \"call_react_agent\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# GRAPH CONSTRUCTION\n",
        "# ============================================================================\n",
        "\n",
        "# Global variable to hold the ReAct agent\n",
        "react_agent = None\n",
        "\n",
        "def create_conversation_graph():\n",
        "    \"\"\"\n",
        "    Build the conversation graph with persistent multi-turn capability.\n",
        "\n",
        "    Graph structure (single conversation with looping):\n",
        "\n",
        "        Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â\n",
        "        Ã¢â€â€š                                                      Ã¢â€â€š\n",
        "        Ã¢â€“Â¼                                                      Ã¢â€â€š\n",
        "      input_node Ã¢â€â‚¬Ã¢â€â‚¬(check command)Ã¢â€â‚¬Ã¢â€â‚¬> call_react_agent        Ã¢â€â€š\n",
        "          Ã¢â€“Â²                              Ã¢â€â€š                     Ã¢â€â€š\n",
        "          Ã¢â€â€š                              Ã¢â€“Â¼                     Ã¢â€â€š\n",
        "          Ã¢â€â€š                         output_node                Ã¢â€â€š\n",
        "          Ã¢â€â€š                              Ã¢â€â€š                     Ã¢â€â€š\n",
        "          Ã¢â€â€š                              Ã¢â€“Â¼                     Ã¢â€â€š\n",
        "          Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬(verbose/quiet)       trim_history Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ\n",
        "\n",
        "          Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬(exit)Ã¢â€â‚¬Ã¢â€â‚¬> END\n",
        "\n",
        "    Key features:\n",
        "    - Single conversation maintained in state.messages\n",
        "    - Command field used for special commands (no sentinel messages!)\n",
        "    - Graph loops back to input_node after each turn\n",
        "    - Verbose/quiet commands route directly back to input\n",
        "    - History automatically trimmed when it grows too long\n",
        "    - No Python loops or checkpointing needed\n",
        "\n",
        "    Returns:\n",
        "        Compiled LangGraph application\n",
        "    \"\"\"\n",
        "    global react_agent\n",
        "\n",
        "    # ========================================================================\n",
        "    # Create the ReAct Agent\n",
        "    # ========================================================================\n",
        "\n",
        "    model = ChatOpenAI(\n",
        "        model=\"gpt-4o\",\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # System message to encourage tool usage\n",
        "    system_message = (\n",
        "        \"You are a helpful assistant. \"\n",
        "        \"If a tool is able to solve a problem you are working on then \"\n",
        "        \"always use it, even if you are able to solve it without using a tool.\"\n",
        "    )\n",
        "\n",
        "    # Create the ReAct agent using the built-in function\n",
        "    # This agent handles the thought/action/observation loop internally\n",
        "    react_agent = create_react_agent(\n",
        "        model=model,\n",
        "        tools=tools,\n",
        "        prompt=system_message\n",
        "    )\n",
        "\n",
        "    print(\"[SYSTEM] ReAct agent created successfully\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # Create the Conversation Wrapper Graph\n",
        "    # ========================================================================\n",
        "\n",
        "    workflow = StateGraph(ConversationState)\n",
        "\n",
        "    # Add all nodes\n",
        "    workflow.add_node(\"input\", input_node)\n",
        "    workflow.add_node(\"call_react_agent\", call_react_agent)\n",
        "    workflow.add_node(\"output\", output_node)\n",
        "    workflow.add_node(\"trim_history\", trim_history)\n",
        "\n",
        "    # Set entry point - conversation always starts at input\n",
        "    workflow.set_entry_point(\"input\")\n",
        "\n",
        "    # Add conditional edge from input based on command field\n",
        "    workflow.add_conditional_edges(\n",
        "        \"input\",\n",
        "        route_after_input,\n",
        "        {\n",
        "            \"call_react_agent\": \"call_react_agent\",\n",
        "            \"input\": \"input\",  # Loop back for verbose/quiet\n",
        "            \"end\": END\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Add linear edges for the main conversation flow\n",
        "    # Agent -> Output -> Trim -> Input (loops back!)\n",
        "    workflow.add_edge(\"call_react_agent\", \"output\")\n",
        "    workflow.add_edge(\"output\", \"trim_history\")\n",
        "    workflow.add_edge(\"trim_history\", \"input\")  # This creates the loop!\n",
        "\n",
        "    # Compile the graph\n",
        "    return workflow.compile()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def visualize_graphs(wrapper_app):\n",
        "    \"\"\"\n",
        "    Generate Mermaid diagrams for both graphs.\n",
        "\n",
        "    Creates:\n",
        "    - langchain_react_agent.png: Internal ReAct agent (thought/action/observation)\n",
        "    - langchain_conversation_graph.png: Conversation loop wrapper\n",
        "\n",
        "    Args:\n",
        "        wrapper_app: Compiled conversation graph\n",
        "    \"\"\"\n",
        "    global react_agent\n",
        "\n",
        "    # Visualize the ReAct agent\n",
        "    try:\n",
        "        react_png = react_agent.get_graph().draw_mermaid_png()\n",
        "        with open(\"langchain_react_agent.png\", \"wb\") as f:\n",
        "            f.write(react_png)\n",
        "        print(\"[SYSTEM] ReAct agent graph saved to 'langchain_react_agent.png'\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARNING] Could not generate ReAct agent visualization: {e}\")\n",
        "\n",
        "    # Visualize the conversation wrapper\n",
        "    try:\n",
        "        wrapper_png = wrapper_app.get_graph().draw_mermaid_png()\n",
        "        with open(\"langchain_conversation_graph.png\", \"wb\") as f:\n",
        "            f.write(wrapper_png)\n",
        "        print(\"[SYSTEM] Conversation graph saved to 'langchain_conversation_graph.png'\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARNING] Could not generate conversation graph visualization: {e}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "async def main():\n",
        "    \"\"\"\n",
        "    Main execution function.\n",
        "\n",
        "    This function:\n",
        "    1. Creates the conversation graph\n",
        "    2. Visualizes the graph structure\n",
        "    3. Initializes the conversation state\n",
        "    4. Invokes the graph ONCE\n",
        "\n",
        "    The graph then runs indefinitely via internal looping (trim_history -> input)\n",
        "    until the user types 'quit' or 'exit'.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"LangGraph ReAct Agent - Persistent Multi-Turn Conversation\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nThis system uses create_react_agent with graph-based looping:\")\n",
        "    print(\"  - Single persistent conversation across all turns\")\n",
        "    print(\"  - History managed automatically (trimmed after 100 messages)\")\n",
        "    print(\"  - Loops via graph edges (no Python loops or checkpointing)\")\n",
        "    print(\"\\nCommands:\")\n",
        "    print(\"  - Type 'quit' or 'exit' to end the conversation\")\n",
        "    print(\"  - Type 'verbose' to enable detailed tracing\")\n",
        "    print(\"  - Type 'quiet' to disable detailed tracing\")\n",
        "    print(\"\\nAvailable tools:\")\n",
        "    print(\"  - get_weather(location): Get weather information\")\n",
        "    print(\"  - get_population(city): Get population data\")\n",
        "    print(\"  - calculate(expression): Evaluate math expressions\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create the conversation graph\n",
        "    app = create_conversation_graph()\n",
        "\n",
        "    # Visualize both graphs\n",
        "    visualize_graphs(app)\n",
        "\n",
        "    # Initialize conversation state\n",
        "    # This state persists across all turns via graph looping\n",
        "    initial_state = {\n",
        "        \"messages\": [],\n",
        "        \"verbose\": True,\n",
        "        \"command\": None\n",
        "    }\n",
        "\n",
        "    print(\"\\n[SYSTEM] Starting conversation...\\n\")\n",
        "\n",
        "    try:\n",
        "        # Invoke the graph ONCE\n",
        "        # The graph will loop internally until user exits\n",
        "        # Each iteration: input -> agent -> output -> trim -> input (loop!)\n",
        "        # Verbose commands: input -> input (direct loop!)\n",
        "        await app.ainvoke(initial_state)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\n[SYSTEM] Interrupted by user (Ctrl+C)\")\n",
        "\n",
        "    print(\"\\n[SYSTEM] Conversation ended. Goodbye!\\n\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ENTRY POINT\n",
        "# ============================================================================\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # asyncio.run() executes main() exactly ONCE\n",
        "#     # The looping happens INSIDE the graph via edges\n",
        "#     asyncio.run(main())\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        asyncio.run(main())\n",
        "    except RuntimeError:\n",
        "        # Running inside Jupyter\n",
        "        import nest_asyncio\n",
        "        nest_asyncio.apply()\n",
        "        asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bw3hsiD810A",
        "outputId": "e77b4546-1d57-41f0-9a9f-b77f7b81c954"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "LangGraph ReAct Agent - Persistent Multi-Turn Conversation\n",
            "================================================================================\n",
            "\n",
            "This system uses create_react_agent with graph-based looping:\n",
            "  - Single persistent conversation across all turns\n",
            "  - History managed automatically (trimmed after 100 messages)\n",
            "  - Loops via graph edges (no Python loops or checkpointing)\n",
            "\n",
            "Commands:\n",
            "  - Type 'quit' or 'exit' to end the conversation\n",
            "  - Type 'verbose' to enable detailed tracing\n",
            "  - Type 'quiet' to disable detailed tracing\n",
            "\n",
            "Available tools:\n",
            "  - get_weather(location): Get weather information\n",
            "  - get_population(city): Get population data\n",
            "  - calculate(expression): Evaluate math expressions\n",
            "================================================================================\n",
            "[SYSTEM] ReAct agent created successfully\n",
            "[SYSTEM] ReAct agent graph saved to 'langchain_react_agent.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4038131512.py:375: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  react_agent = create_react_agent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SYSTEM] Conversation graph saved to 'langchain_conversation_graph.png'\n",
            "\n",
            "[SYSTEM] Starting conversation...\n",
            "\n",
            "\n",
            "================================================================================\n",
            "NODE: input_node\n",
            "================================================================================\n",
            "\n",
            "You: what is the weather and population of washington dc today\n",
            "[DEBUG] User input: what is the weather and population of washington dc today\n",
            "[DEBUG] Routing to call_react_agent\n",
            "\n",
            "================================================================================\n",
            "NODE: call_react_agent\n",
            "================================================================================\n",
            "[DEBUG] Invoking ReAct agent with 1 messages in history\n",
            "[DEBUG] Agent generated 4 new messages\n",
            "[DEBUG] Tool calls: ['get_weather', 'get_population']\n",
            "[DEBUG] Response preview: The current weather in Washington DC is sunny with a temperature of 72°F and light winds. The popula...\n",
            "\n",
            "================================================================================\n",
            "NODE: output_node\n",
            "================================================================================\n",
            "\n",
            "Assistant: The current weather in Washington DC is sunny with a temperature of 72°F and light winds. The population of Washington DC is approximately 1 million people.\n",
            "\n",
            "================================================================================\n",
            "NODE: input_node\n",
            "================================================================================\n",
            "\n",
            "You: exit\n",
            "[DEBUG] Exit command received\n",
            "[DEBUG] Routing to END (exit requested)\n",
            "\n",
            "[SYSTEM] Conversation ended. Goodbye!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ToolNode uses the asyncio event loop which calls the async functions. The await inside these functions allows other tools to run concurrently, so multiple tools can be called and run at the same time. The kind of tools that would benefit the most are input/output functions, such as API calls, HTTP scraping, LLM calls to different models, etc. because these kind of tasks spend most of their time waiting for the output to be generated, so async concurrency gives a big boost in speed.\n",
        "2. The two programs handle special inputs in the input_node function, where state flags are set depending on the input. In the routing after parsing the input, these flags are checked and different actions (such as termination or asking for input again) are taken without calling the model or adding to chat history.\n",
        "3. The main difference between ToolNode and the react agent is how the model interacts with the tools. In the ToolNode graph, the tools can be explicitly seen in a predefined loop. In the react agent graph however, it seems more black-box where the internal loop is hidden as the agent continuously reasons about which tools to use depending on the current state and the desired output.  \n",
        "4. One example of when the LangChang react agent is too restrictive is when we want to execute a task of running 100 API calls and summarizing results. The toolnode approach is much better in this scenario because of its ability to run tools in parallel, rather than the react agent which would have to make 100 separate LLM calls sequentially. The react agent would be more beneficial in tasks where reasoning is needed."
      ],
      "metadata": {
        "id": "S1lBrO6U-pj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4"
      ],
      "metadata": {
        "id": "FcojThLl8xej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube-transcript-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDDkaAW-FDFj",
        "outputId": "dc6790ec-0fab-40b2-9ee5-e4cf2ecff28d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-1.2.4-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (2026.1.4)\n",
            "Downloading youtube_transcript_api-1.2.4-py3-none-any.whl (485 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/485.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.2/485.2 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-1.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from langchain.tools import tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "@tool\n",
        "def get_youtube_transcript(video_id: str) -> str:\n",
        "    \"\"\"Fetch the transcript of a YouTube video by video ID.\"\"\"\n",
        "    ytapi = YouTubeTranscriptApi()\n",
        "    transcript = ytapi.fetch(video_id)\n",
        "    text = \" \".join(t.text for t in transcript)\n",
        "    return text\n",
        "    # transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "    # return \" \".join([entry['text'] for entry in transcript])\n",
        "\n",
        "# Create agent with tool\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "agent = create_react_agent(llm, [get_youtube_transcript])\n",
        "\n",
        "# Test it\n",
        "result = agent.invoke({\n",
        "    \"messages\": [(\"user\", \"Get the transcript for video dQw4w9WgXcQ and summarize it\")]\n",
        "})\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LL8heD6Xy0i",
        "outputId": "7bc72c97-d793-4af7-8937-42c51bcbe1a7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3533126474.py:19: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  agent = create_react_agent(llm, [get_youtube_transcript])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'messages': [HumanMessage(content='Get the transcript for video dQw4w9WgXcQ and summarize it', additional_kwargs={}, response_metadata={}, id='027ef7d4-e510-4d02-b5aa-de1c6d51f05d'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 72, 'total_tokens': 100, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f4ae844694', 'id': 'chatcmpl-D7kubFBvdD5aYkwE3xXQ1jN8nrB3D', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c4863-59e7-7d72-8811-3310680d726a-0', tool_calls=[{'name': 'get_youtube_transcript', 'args': {'video_id': 'dQw4w9WgXcQ'}, 'id': 'call_Om00RyHZRGR47btCif7ti8lq', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 72, 'output_tokens': 28, 'total_tokens': 100, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content=\"[♪♪♪] ♪ We're no strangers to love ♪ ♪ You know the rules\\nand so do I ♪ ♪ A full commitment's\\nwhat I'm thinking of ♪ ♪ You wouldn't get this\\nfrom any other guy ♪ ♪ I just wanna tell you\\nhow I'm feeling ♪ ♪ Gotta make you understand ♪ ♪ Never gonna give you up ♪ ♪ Never gonna let you down ♪ ♪ Never gonna run around\\nand desert you ♪ ♪ Never gonna make you cry ♪ ♪ Never gonna say goodbye ♪ ♪ Never gonna tell a lie\\nand hurt you ♪ ♪ We've known each other\\nfor so long ♪ ♪ Your heart's been aching\\nbut you're too shy to say it ♪ ♪ Inside we both know\\nwhat's been going ♪ ♪ We know the game\\nand we're gonna play it ♪ ♪ And if you ask me\\nhow I'm feeling ♪ ♪ Don't tell me\\nyou're too blind to see ♪ ♪ Never gonna give you up ♪ ♪ Never gonna let you down ♪ ♪ Never gonna run around\\nand desert you ♪ ♪ Never gonna make you cry ♪ ♪ Never gonna say goodbye ♪ ♪ Never gonna tell a lie\\nand hurt you ♪ ♪ Never gonna give you up ♪ ♪ Never gonna let you down ♪ ♪ Never gonna run around\\nand desert you ♪ ♪ Never gonna make you cry ♪ ♪ Never gonna say goodbye ♪ ♪ Never gonna tell a lie\\nand hurt you ♪ ♪ (Ooh, give you up) ♪ ♪ (Ooh, give you up) ♪ ♪ Never gonna give,\\nnever gonna give ♪ ♪ (Give you up) ♪ ♪ Never gonna give,\\nnever gonna give ♪ ♪ (Give you up) ♪ ♪ We've known each other\\nfor so long ♪ ♪ Your heart's been aching\\nbut you're too shy to say it ♪ ♪ Inside we both know\\nwhat's been going ♪ ♪ We know the game\\nand we're gonna play it ♪ ♪ I just wanna tell you\\nhow I'm feeling ♪ ♪ Gotta make you understand ♪ ♪ Never gonna give you up ♪ ♪ Never gonna let you down ♪ ♪ Never gonna run around\\nand desert you ♪ ♪ Never gonna make you cry ♪ ♪ Never gonna say goodbye ♪ ♪ Never gonna tell a lie\\nand hurt you ♪ ♪ Never gonna give you up ♪ ♪ Never gonna let you down ♪ ♪ Never gonna run around\\nand desert you ♪ ♪ Never gonna make you cry ♪ ♪ Never gonna say goodbye ♪ ♪ Never gonna tell a lie\\nand hurt you ♪ ♪ Never gonna give you up ♪ ♪ Never gonna let you down ♪ ♪ Never gonna run around\\nand desert you ♪ ♪ Never gonna make you cry ♪ ♪ Never gonna say goodbye ♪ ♪ Never gonna tell a lie\\nand hurt you ♪\", name='get_youtube_transcript', id='0e0642f6-02e8-4c56-885e-71adfba688ac', tool_call_id='call_Om00RyHZRGR47btCif7ti8lq'), AIMessage(content='The transcript for the video \"Never Gonna Give You Up\" by Rick Astley consists entirely of the song\\'s lyrics. Below is a summary of the song\\'s main themes:\\n\\n### Summary:\\nThe song expresses a deep commitment to love and loyalty. The singer promises never to betray or abandon the beloved, emphasizing the importance of trust and honesty in a relationship. The repeated refrain conveys a sense of unwavering dedication, assuring the listener that they will never be let down, made to cry, or deceived. The song depicts an understanding between two individuals who are aware of their feelings yet may be hesitant to express them openly.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 125, 'prompt_tokens': 650, 'total_tokens': 775, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f4ae844694', 'id': 'chatcmpl-D7kueOdQvUQoRC54HYAnXu1ie8S9O', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c4863-666d-76d2-9170-013f7638cb59-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 650, 'output_tokens': 125, 'total_tokens': 775, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from langchain.tools import tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\n",
        "\n",
        "@tool\n",
        "def get_youtube_transcript(video_id: str) -> str:\n",
        "    \"\"\"Fetch the transcript of a YouTube video by video ID.\"\"\"\n",
        "    ytapi = YouTubeTranscriptApi()\n",
        "    transcript = ytapi.fetch(video_id)\n",
        "    text = \" \".join(t.text for t in transcript)\n",
        "    return text\n",
        "\n",
        "@tool\n",
        "def summarize_transcript(transcript: str) -> str:\n",
        "    \"\"\"Summarize a YouTube transcript.\"\"\"\n",
        "    prompt = f\"Summarize the following transcript:\\n\\n{transcript}\"\n",
        "    return llm.invoke(prompt).content\n",
        "\n",
        "@tool\n",
        "def extract_key_concepts(transcript: str) -> str:\n",
        "    \"\"\"Extract key concepts from a transcript.\"\"\"\n",
        "    prompt = f\"List the key concepts or topics discussed:\\n\\n{transcript}\"\n",
        "    return llm.invoke(prompt).content\n",
        "\n",
        "@tool\n",
        "def generate_quiz(transcript: str) -> str:\n",
        "    \"\"\"Generate quiz questions from a transcript.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Create 5 quiz questions (with answers) based on this transcript:\n",
        "\n",
        "    {transcript}\n",
        "    \"\"\"\n",
        "    return llm.invoke(prompt).content\n",
        "\n",
        "# Create agent with tool\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "tools = [\n",
        "    get_youtube_transcript,\n",
        "    summarize_transcript,\n",
        "    extract_key_concepts,\n",
        "    generate_quiz,\n",
        "]\n",
        "\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def run_agent(user_query: str):\n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a helpful AI that can analyze YouTube videos using tools.\"),\n",
        "        HumanMessage(content=user_query)\n",
        "    ]\n",
        "\n",
        "    print(f\"User: {user_query}\\n\")\n",
        "\n",
        "    for iteration in range(6):\n",
        "        print(f\"--- Iteration {iteration+1} ---\")\n",
        "        response = llm_with_tools.invoke(messages)\n",
        "\n",
        "        if response.tool_calls:\n",
        "            print(f\"LLM wants to call {len(response.tool_calls)} tool(s)\")\n",
        "            messages.append(response)\n",
        "\n",
        "            for tool_call in response.tool_calls:\n",
        "                name = tool_call[\"name\"]\n",
        "                args = tool_call[\"args\"]\n",
        "\n",
        "                print(f\"Tool call: {name}\")\n",
        "                print(f\"Args: {args}\")\n",
        "\n",
        "                if name == \"get_youtube_transcript\":\n",
        "                    result = get_youtube_transcript.invoke(args)\n",
        "                elif name == \"summarize_transcript\":\n",
        "                    result = summarize_transcript.invoke(args)\n",
        "                elif name == \"extract_key_concepts\":\n",
        "                    result = extract_key_concepts.invoke(args)\n",
        "                elif name == \"generate_quiz\":\n",
        "                    result = generate_quiz.invoke(args)\n",
        "                else:\n",
        "                    result = f\"Unknown tool: {name}\"\n",
        "\n",
        "                print(\"Result (truncated):\", result[:300], \"\\n\")\n",
        "\n",
        "                messages.append(ToolMessage(\n",
        "                    content=result,\n",
        "                    tool_call_id=tool_call[\"id\"]\n",
        "                ))\n",
        "        else:\n",
        "            print(f\"Assistant: {response.content}\\n\")\n",
        "            return response.content\n",
        "\n",
        "    return \"Max iterations reached\"\n",
        "\n",
        "# Test it\n",
        "if __name__ == \"__main__\":\n",
        "    # Test query that requires tool use\n",
        "    print(\"=\"*60)\n",
        "    print(\"TEST 1: Query requiring tool\")\n",
        "    print(\"=\"*60)\n",
        "    run_agent(\"Get the transcript for the video dQw4w9WgXcQ\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TEST 2: Query not requiring tool\")\n",
        "    print(\"=\"*60)\n",
        "    run_agent(\"Say hello!\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TEST 3: Multiple tool calls\")\n",
        "    print(\"=\"*60)\n",
        "    run_agent(\"Summarize the transcript of video 1bUy-1hGZpI and generate quiz questions to test my knowledge\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW39uM78TcJl",
        "outputId": "30610ec7-ad51-48da-ceed-b4da454889b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TEST 1: Query requiring tool\n",
            "============================================================\n",
            "User: Get the transcript for the video dQw4w9WgXcQ\n",
            "\n",
            "--- Iteration 1 ---\n",
            "LLM wants to call 1 tool(s)\n",
            "Tool call: get_youtube_transcript\n",
            "Args: {'video_id': 'dQw4w9WgXcQ'}\n",
            "Result (truncated): [♪♪♪] ♪ We're no strangers to love ♪ ♪ You know the rules\n",
            "and so do I ♪ ♪ A full commitment's\n",
            "what I'm thinking of ♪ ♪ You wouldn't get this\n",
            "from any other guy ♪ ♪ I just wanna tell you\n",
            "how I'm feeling ♪ ♪ Gotta make you understand ♪ ♪ Never gonna give you up ♪ ♪ Never gonna let you down ♪ ♪ Never g \n",
            "\n",
            "--- Iteration 2 ---\n",
            "Assistant: The transcript for the video \"Never Gonna Give You Up\" by Rick Astley is as follows:\n",
            "\n",
            "[♪♪♪]\n",
            "♪ We're no strangers to love ♪  \n",
            "♪ You know the rules and so do I ♪  \n",
            "♪ A full commitment's what I'm thinking of ♪  \n",
            "♪ You wouldn't get this from any other guy ♪  \n",
            "♪ I just wanna tell you how I'm feeling ♪  \n",
            "♪ Gotta make you understand ♪  \n",
            "\n",
            "♪ Never gonna give you up ♪  \n",
            "♪ Never gonna let you down ♪  \n",
            "♪ Never gonna run around and desert you ♪  \n",
            "♪ Never gonna make you cry ♪  \n",
            "♪ Never gonna say goodbye ♪  \n",
            "♪ Never gonna tell a lie and hurt you ♪  \n",
            "\n",
            "♪ We've known each other for so long ♪  \n",
            "♪ Your heart's been aching but you're too shy to say it ♪  \n",
            "♪ Inside we both know what's been going ♪  \n",
            "♪ We know the game and we're gonna play it ♪  \n",
            "♪ And if you ask me how I'm feeling ♪  \n",
            "♪ Don't tell me you're too blind to see ♪  \n",
            "\n",
            "♪ Never gonna give you up ♪  \n",
            "♪ Never gonna let you down ♪  \n",
            "♪ Never gonna run around and desert you ♪  \n",
            "♪ Never gonna make you cry ♪  \n",
            "♪ Never gonna say goodbye ♪  \n",
            "♪ Never gonna tell a lie and hurt you ♪  \n",
            "\n",
            "♪ Never gonna give you up ♪  \n",
            "♪ Never gonna let you down ♪  \n",
            "♪ Never gonna run around and desert you ♪  \n",
            "♪ Never gonna make you cry ♪  \n",
            "♪ Never gonna say goodbye ♪  \n",
            "♪ Never gonna tell a lie and hurt you ♪  \n",
            "\n",
            "♪ (Ooh, give you up) ♪  \n",
            "♪ (Ooh, give you up) ♪  \n",
            "♪ Never gonna give, never gonna give ♪  \n",
            "♪ (Give you up) ♪  \n",
            "♪ Never gonna give, never gonna give ♪  \n",
            "♪ (Give you up) ♪  \n",
            "\n",
            "♪ We've known each other for so long ♪  \n",
            "♪ Your heart's been aching but you're too shy to say it ♪  \n",
            "♪ Inside we both know what's been going ♪  \n",
            "♪ We know the game and we're gonna play it ♪  \n",
            "♪ I just wanna tell you how I'm feeling ♪  \n",
            "♪ Gotta make you understand ♪  \n",
            "\n",
            "♪ Never gonna give you up ♪  \n",
            "♪ Never gonna let you down ♪  \n",
            "♪ Never gonna run around and desert you ♪  \n",
            "♪ Never gonna make you cry ♪  \n",
            "♪ Never gonna say goodbye ♪  \n",
            "♪ Never gonna tell a lie and hurt you ♪  \n",
            "\n",
            "♪ Never gonna give you up ♪  \n",
            "♪ Never gonna let you down ♪  \n",
            "♪ Never gonna run around and desert you ♪  \n",
            "♪ Never gonna make you cry ♪  \n",
            "♪ Never gonna say goodbye ♪  \n",
            "♪ Never gonna tell a lie and hurt you ♪  \n",
            "\n",
            "Feel free to ask if you need any further analysis or summary!\n",
            "\n",
            "\n",
            "============================================================\n",
            "TEST 2: Query not requiring tool\n",
            "============================================================\n",
            "User: Say hello!\n",
            "\n",
            "--- Iteration 1 ---\n",
            "Assistant: Hello! How can I assist you today?\n",
            "\n",
            "\n",
            "============================================================\n",
            "TEST 3: Multiple tool calls\n",
            "============================================================\n",
            "User: Summarize the transcript of video 1bUy-1hGZpI and generate quiz questions to test my knowledge\n",
            "\n",
            "--- Iteration 1 ---\n",
            "LLM wants to call 2 tool(s)\n",
            "Tool call: get_youtube_transcript\n",
            "Args: {'video_id': '1bUy-1hGZpI'}\n",
            "Result (truncated): now stop me if you've heard this one before but there are a lot of large language models available today and they have their own capabilities and specialities what if I prefer to use one llm to interpret some user queries in my business application but a whole other llm to author a response to those \n",
            "\n",
            "Tool call: generate_quiz\n",
            "Args: {'transcript': ''}\n",
            "Result (truncated): I don't have access to a specific transcript to create quiz questions from. If you can provide text or key details from the transcript you'd like me to use, I can certainly help you create quiz questions based on that information! \n",
            "\n",
            "--- Iteration 2 ---\n",
            "LLM wants to call 1 tool(s)\n",
            "Tool call: summarize_transcript\n",
            "Args: {'transcript': \"now stop me if you've heard this one before but there are a lot of large language models available today and they have their own capabilities and specialities what if I prefer to use one llm to interpret some user queries in my business application but a whole other llm to author a response to those queries well that scenario is exactly what Lang chain caters to Lang chain is an open-source orchestration framework for the development of applications that use large language models and it comes in both Python and JavaScript libraries it's it's essentially a generic interface for nearly any llm so you have a centralized development environment to build your large language model applications and then integrate them with stuff like data sources and software workflows now when it was launched by Harrison Chase in October 2022 Lang chain enjoyed a meteoric rise and by June of the following year it was the single fastest growing open- source project on GitHub and while the Lang chain hype train has uh slightly cooled a little bit there's plenty of utility here so let's take a look at its components so what makes up Lang chain well Lang chain streamlines the programming of llm applications through something called abstractions now what do I mean by that well your thermostat that allows you to control the temperature in your home with without needing to understand all the complex circuitary that this entails we just set the temperature that's an abstraction so Lang chains abstractions represent common steps and Concepts necessary to work with language models and they can be chained together to create applications minimizing the amount of code required to execute complex NLP tasks so let's start with the llm module now nearly any LM LM can be used in Lang chain you just need an API key the llm class is designed to provide a standard interface for all models so pick an llm of your choice be that a closed Source One like gp4 or an Open Source One like llama 2 or this being Lang chain pick both okay what else we got we have prompts now prompts are the instructions given to a large language model and the prompt template class in Lang chain formalizes the composition of prompts without the need to manually hardcode context and queries a prompt template can contain instructions like uh do not use technical terms in your response that would be a good one or it could be a set of examples to guide its responses that's called f shot prompting or it could specify an output format now chains as the name implies are the core of Lang chain workflows they combine llms with other components creating applications by executing a sequence of functions so let's say our application that needs to first of all retrieve data from a website then it needs to summarize the text it gets back and then finally it needs to use that summary to answer User submitted questions that's a sequential chain where the output of one function access the input to the next and each function in the chain could use different prompts different parameters and even different models now to achieve certain tasks llms might need to access specific external data sources that are not included in the training data set of the llm itself so things like internal documents or emails that sort of thing now Lang chain collectively refers to this sort of documentation as indexes and there are a number of them so let's take a look at a few now one of them is called a document loader now document loaders they work with thirdparty applications for importing data sources from sources like file storage services so think Dropbox or Google drive or web content from like YouTube transcripts or collaboration tools like air table or databases like pandas and mongod DB there's also support for vector databases as well now unlike traditional structured databases Vector databases represent data points by converting them into something called Vector embeddings which are numerical representations in the form of vectors with a fixed number of dimensions and you can store a lot of information in this format as as it's a very efficient means of retrieval there are also something called text Splitters which can be very useful as well because they can split text up into small semantically meaningful chunks that can then be combined using the methods and parameters of your choosing Now llms by default don't really have any long-term memory of Prior conversations unless you happen to pass the chat history in as an input to your query but Lang chain solves this problem with simple utilities for adding in memory into your application and you have options retain for retaining like the entire High conversations through two options to just retain a summarization of the conversation that we've had so far and then finally the last one we'll look at are agents now agents can use a given language model as a reasoning engine to determine which actions to take and when building a chain for an agent you'll want to include inputs like a list of the available tools that it should use uh the user input like the prompts and the queries and then any other relevant previously executed steps so how can we put all of this to work for our applications well let's talk about a few Lang chain use cases now obviously we have chatbots Lang chain can be used to provide proper context for the specific use of a chatbot and to integrate chatbots into existing communication channels and workflows with their own apis we also have summarization language model can be tasked with summarizing many types of text from breaking down complex academic papers and transcripts to providing just a digest of incoming emails we've also seen lots of examples where this is used for question answering so using specific documents or specialized knowledge basis llms can retrieve the relevant information from the storage and then articulate helpful answers using the information that would otherwise not have been in their training data set and uh yeah this is a good one data augmentation llms can be used to generate synthetic data for use of machine learning so for example llm can be trained to generate additional samples that closely resemble the real data points in a training data set and there are of course virtual agents as we already started to discuss integrated with the the right workflows Lang chains agent modules can use an llm to autonomously determine the next steps and then take the action that it needs to complete that step using something called RPA or robotic process automation Lang chain is open source and free to use there are also related Frameworks like Lang serve for creating chains as rest apis and Lang Smith which provides tools to monitor evaluate and debug applications essentially Lang Chain's tools and apis simplify the process of building applications that make use of large language models if you have any questions please drop us a line below and if you want to see more videos like this in the future please like And subscribe thanks for watching.\"}\n",
            "Result (truncated): The transcript discusses LangChain, an open-source orchestration framework designed for developing applications using large language models (LLMs). It supports both Python and JavaScript libraries and provides a centralized environment for creating LLM applications, allowing the integration of vario \n",
            "\n",
            "--- Iteration 3 ---\n",
            "LLM wants to call 1 tool(s)\n",
            "Tool call: generate_quiz\n",
            "Args: {'transcript': 'The transcript discusses LangChain, an open-source orchestration framework designed for developing applications using large language models (LLMs). It supports both Python and JavaScript libraries and provides a centralized environment for creating LLM applications, allowing the integration of various data sources and workflows.\\n\\nLaunched by Harrison Chase in October 2022, LangChain quickly became the fastest-growing open-source project on GitHub. Its utility remains strong, offering several components that streamline LLM application development through abstractions.\\n\\nKey components include:\\n1. **LLM Module**: Offers a standard interface for using different LLMs, whether closed-source or open-source.\\n2. **Prompt Templates**: Allow structured composition of prompts for LLMs, helping guide responses without hardcoding.\\n3. **Chains**: Enable sequential execution of functions where outputs from one step can serve as inputs to another, potentially utilizing different models for different tasks.\\n4. **Indexes**: Facilitate access to external data sources, such as document loaders for file storage services and vector databases that convert data into numerical representations for efficient retrieval.\\n5. **Memory Utilities**: Allow applications to remember prior conversations either in full or as summaries.\\n6. **Agents**: Use LLMs as reasoning engines to determine actions in a workflow.\\n\\nPotential use cases for LangChain include building chatbots, summarizing texts, answering questions based on specific knowledge bases, generating synthetic data for machine learning, and using virtual agents for automated processes.\\n\\nLangChain is open-source and free to use, with related frameworks like LangServe for creating REST APIs and LangSmith for monitoring and debugging applications, simplifying the development process for LLM-based applications. The transcript concludes by inviting questions and encouraging viewers to engage with the content.'}\n",
            "Result (truncated): ### Quiz Questions\n",
            "\n",
            "1. **What is LangChain?**\n",
            "   - A) A proprietary software for language modeling  \n",
            "   - B) An open-source orchestration framework for developing applications using large language models (LLMs)  \n",
            "   - C) A database management system  \n",
            "   - D) A text editing tool  \n",
            "   **Answer:** B)  \n",
            "\n",
            "--- Iteration 4 ---\n",
            "Assistant: ### Summary of the Transcript\n",
            "\n",
            "The transcript discusses **LangChain**, an open-source orchestration framework designed for developing applications using large language models (LLMs). It supports both Python and JavaScript libraries and provides a centralized environment for creating LLM applications, allowing the integration of various data sources and workflows.\n",
            "\n",
            "Launched by **Harrison Chase** in October 2022, LangChain quickly became the fastest-growing open-source project on GitHub. Its utility remains strong, offering several components that streamline LLM application development through abstractions.\n",
            "\n",
            "Key components include:\n",
            "1. **LLM Module**: Offers a standard interface for using different LLMs, whether closed-source or open-source.\n",
            "2. **Prompt Templates**: Allow structured composition of prompts for LLMs, helping guide responses without hardcoding.\n",
            "3. **Chains**: Enable sequential execution of functions where outputs from one step can serve as inputs to another, potentially utilizing different models for different tasks.\n",
            "4. **Indexes**: Facilitate access to external data sources, such as document loaders for file storage services and vector databases that convert data into numerical representations for efficient retrieval.\n",
            "5. **Memory Utilities**: Allow applications to remember prior conversations either in full or as summaries.\n",
            "6. **Agents**: Use LLMs as reasoning engines to determine actions in a workflow.\n",
            "\n",
            "Potential use cases for LangChain include building chatbots, summarizing texts, answering questions based on specific knowledge bases, generating synthetic data for machine learning, and using virtual agents for automated processes.\n",
            "\n",
            "LangChain is open-source and free to use, with related frameworks like **LangServe** for creating REST APIs and **LangSmith** for monitoring and debugging applications, simplifying the development process for LLM-based applications. The transcript concludes by inviting questions and encouraging viewers to engage with the content.\n",
            "\n",
            "---\n",
            "\n",
            "### Quiz Questions\n",
            "\n",
            "1. **What is LangChain?**\n",
            "   - A) A proprietary software for language modeling  \n",
            "   - B) An open-source orchestration framework for developing applications using large language models (LLMs)  \n",
            "   - C) A database management system  \n",
            "   - D) A text editing tool  \n",
            "   **Answer:** B) An open-source orchestration framework for developing applications using large language models (LLMs)\n",
            "\n",
            "2. **Who launched LangChain and when?**\n",
            "   - A) John Doe in January 2023  \n",
            "   - B) Harrison Chase in October 2022  \n",
            "   - C) Jane Smith in December 2021  \n",
            "   - D) Michael Brown in March 2020  \n",
            "   **Answer:** B) Harrison Chase in October 2022\n",
            "\n",
            "3. **Which key component of LangChain helps in structuring prompts for LLMs?**\n",
            "   - A) Chains  \n",
            "   - B) Prompt Templates  \n",
            "   - C) Agents  \n",
            "   - D) Indexes  \n",
            "   **Answer:** B) Prompt Templates\n",
            "\n",
            "4. **What functionality do Indexes provide in LangChain?**\n",
            "   - A) They allow the sequential execution of functions.  \n",
            "   - B) They facilitate access to external data sources.  \n",
            "   - C) They generate synthetic data for machine learning.  \n",
            "   - D) They maintain memory of past conversations.  \n",
            "   **Answer:** B) They facilitate access to external data sources.\n",
            "\n",
            "5. **Which of the following is a potential use case for LangChain?**\n",
            "   - A) Operating systems management  \n",
            "   - B) Building chatbots and summarizing texts  \n",
            "   - C) Video game design  \n",
            "   - D) Image processing  \n",
            "   **Answer:** B) Building chatbots and summarizing texts\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from langchain.tools import tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_core.messages import ToolMessage\n",
        "import asyncio\n",
        "import time\n",
        "from typing import TypedDict, Annotated, Sequence, Literal\n",
        "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_youtube_transcript(video_id: str) -> str:\n",
        "    \"\"\"Fetch the transcript of a YouTube video by video ID.\"\"\"\n",
        "    ytapi = YouTubeTranscriptApi()\n",
        "    transcript = ytapi.fetch(video_id)\n",
        "    text = \" \".join(t.text for t in transcript)\n",
        "    return text\n",
        "    # transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "    # return \" \".join([entry['text'] for entry in transcript])\n",
        "\n",
        "tools = [get_youtube_transcript]\n",
        "\n",
        "class ConversationState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    verbose: bool\n",
        "    command: str\n",
        "    transcript: str  # <-- NEW\n",
        "\n",
        "def store_transcript_node(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Extract last tool output and store transcript in state.\n",
        "    \"\"\"\n",
        "    for msg in reversed(state[\"messages\"]):\n",
        "        if isinstance(msg, ToolMessage) and msg.name == \"get_youtube_transcript\":\n",
        "            return {\"transcript\": msg.content}\n",
        "    return {}\n",
        "\n",
        "\n",
        "def input_node(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Get input from the user and add it to the conversation.\n",
        "\n",
        "    This node:\n",
        "    - Prompts the user for input\n",
        "    - Handles special commands (quit, exit, verbose, quiet)\n",
        "    - Adds user message to conversation history (for real messages only)\n",
        "    - Sets command field for special commands\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with new user message or command\n",
        "    \"\"\"\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"NODE: input_node\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    # Get user input\n",
        "    user_input = input(\"\\nYou: \").strip()\n",
        "\n",
        "    # Handle exit commands\n",
        "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Exit command received\")\n",
        "        # Set command field, don't add to messages\n",
        "        return {\"command\": \"exit\"}\n",
        "\n",
        "    # Handle verbose toggle\n",
        "    if user_input.lower() == \"verbose\":\n",
        "        print(\"[SYSTEM] Verbose mode enabled\")\n",
        "        # Set command field and update verbose flag\n",
        "        return {\"command\": \"verbose\", \"verbose\": True}\n",
        "\n",
        "    if user_input.lower() == \"quiet\":\n",
        "        print(\"[SYSTEM] Verbose mode disabled\")\n",
        "        # Set command field and update verbose flag\n",
        "        return {\"command\": \"quiet\", \"verbose\": False}\n",
        "\n",
        "    # Add user message to conversation history\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(f\"[DEBUG] User input: {user_input}\")\n",
        "\n",
        "    # Clear command field and add message\n",
        "    return {\"command\": None, \"messages\": [HumanMessage(content=user_input)]}\n",
        "\n",
        "\n",
        "def call_react_agent(state: ConversationState) -> ConversationState:\n",
        "    global react_agent\n",
        "\n",
        "    messages = list(state[\"messages\"])\n",
        "\n",
        "    # Inject transcript as system context\n",
        "    if \"transcript\" in state and state[\"transcript\"]:\n",
        "        messages = [\n",
        "            SystemMessage(content=f\"VIDEO TRANSCRIPT:\\n{state['transcript'][:12000]}\"),\n",
        "            *messages\n",
        "        ]\n",
        "\n",
        "    result = react_agent.invoke({\"messages\": messages})\n",
        "    new_messages = result[\"messages\"][len(messages):]\n",
        "    return {\"messages\": new_messages}\n",
        "\n",
        "\n",
        "def output_node(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Display the assistant's final response to the user.\n",
        "\n",
        "    This node:\n",
        "    - Extracts the last AI message from the conversation\n",
        "    - Prints it to the console\n",
        "    - Returns empty dict (no state changes)\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Empty dict (no state modifications)\n",
        "    \"\"\"\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"NODE: output_node\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    # Find the last AI message in the conversation\n",
        "    # (there may be tool messages mixed in)\n",
        "    last_ai_message = None\n",
        "    for msg in reversed(state[\"messages\"]):\n",
        "        if isinstance(msg, AIMessage) and msg.content:\n",
        "            last_ai_message = msg\n",
        "            break\n",
        "\n",
        "    if last_ai_message:\n",
        "        print(f\"\\nAssistant: {last_ai_message.content}\")\n",
        "    else:\n",
        "        print(\"\\n[WARNING] No assistant response found\")\n",
        "\n",
        "    return {}\n",
        "\n",
        "\n",
        "def trim_history(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Manage conversation history length to prevent unlimited growth.\n",
        "\n",
        "    Strategy:\n",
        "    - Keep the system message (if present)\n",
        "    - Keep the most recent 100 messages\n",
        "    - This allows ~50 conversation turns (user + assistant pairs)\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with trimmed message history (if needed)\n",
        "    \"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    max_messages = 100\n",
        "\n",
        "    # Only trim if we've exceeded the limit\n",
        "    if len(messages) > max_messages:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(f\"\\n[DEBUG] History length: {len(messages)} messages\")\n",
        "            print(f\"[DEBUG] Trimming to most recent {max_messages} messages\")\n",
        "\n",
        "        # Preserve system message if it exists at the start\n",
        "        if messages and isinstance(messages[0], SystemMessage):\n",
        "            # Keep system message + last (max_messages - 1) messages\n",
        "            trimmed = [messages[0]] + list(messages[-(max_messages - 1):])\n",
        "            if state.get(\"verbose\", True):\n",
        "                print(f\"[DEBUG] Preserved system message + {max_messages - 1} recent messages\")\n",
        "        else:\n",
        "            # Just keep the last max_messages\n",
        "            trimmed = list(messages[-max_messages:])\n",
        "            if state.get(\"verbose\", True):\n",
        "                print(f\"[DEBUG] Kept {max_messages} most recent messages\")\n",
        "\n",
        "        return {\"messages\": trimmed}\n",
        "\n",
        "    # No trimming needed\n",
        "    return {}\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ROUTING LOGIC\n",
        "# ============================================================================\n",
        "\n",
        "def route_after_input(state: ConversationState) -> Literal[\"call_react_agent\", \"end\", \"input\"]:\n",
        "    \"\"\"\n",
        "    Determine where to route after input based on command field.\n",
        "\n",
        "    Logic:\n",
        "    - If command is \"exit\", route to END\n",
        "    - If command is \"verbose\" or \"quiet\", route back to input\n",
        "    - Otherwise (command is None), route to the ReAct agent\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        \"end\" to terminate, \"input\" for verbose toggle, \"call_react_agent\" to continue\n",
        "    \"\"\"\n",
        "    command = state.get(\"command\")\n",
        "\n",
        "    # Check for exit command\n",
        "    if command == \"exit\":\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Routing to END (exit requested)\")\n",
        "        return \"end\"\n",
        "\n",
        "    # Check for verbose toggle commands - route back to input\n",
        "    if command in [\"verbose\", \"quiet\"]:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Routing back to input (verbose toggle)\")\n",
        "        return \"input\"\n",
        "\n",
        "    # Normal message - route to agent\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"[DEBUG] Routing to call_react_agent\")\n",
        "    return \"call_react_agent\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# GRAPH CONSTRUCTION\n",
        "# ============================================================================\n",
        "\n",
        "# Global variable to hold the ReAct agent\n",
        "react_agent = None\n",
        "\n",
        "def create_conversation_graph():\n",
        "    \"\"\"\n",
        "    Build the conversation graph with persistent multi-turn capability.\n",
        "\n",
        "    Graph structure (single conversation with looping):\n",
        "\n",
        "        Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â\n",
        "        Ã¢â€â€š                                                      Ã¢â€â€š\n",
        "        Ã¢â€“Â¼                                                      Ã¢â€â€š\n",
        "      input_node Ã¢â€â‚¬Ã¢â€â‚¬(check command)Ã¢â€â‚¬Ã¢â€â‚¬> call_react_agent        Ã¢â€â€š\n",
        "          Ã¢â€“Â²                              Ã¢â€â€š                     Ã¢â€â€š\n",
        "          Ã¢â€â€š                              Ã¢â€“Â¼                     Ã¢â€â€š\n",
        "          Ã¢â€â€š                         output_node                Ã¢â€â€š\n",
        "          Ã¢â€â€š                              Ã¢â€â€š                     Ã¢â€â€š\n",
        "          Ã¢â€â€š                              Ã¢â€“Â¼                     Ã¢â€â€š\n",
        "          Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬(verbose/quiet)       trim_history Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ\n",
        "\n",
        "          Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬(exit)Ã¢â€â‚¬Ã¢â€â‚¬> END\n",
        "\n",
        "    Key features:\n",
        "    - Single conversation maintained in state.messages\n",
        "    - Command field used for special commands (no sentinel messages!)\n",
        "    - Graph loops back to input_node after each turn\n",
        "    - Verbose/quiet commands route directly back to input\n",
        "    - History automatically trimmed when it grows too long\n",
        "    - No Python loops or checkpointing needed\n",
        "\n",
        "    Returns:\n",
        "        Compiled LangGraph application\n",
        "    \"\"\"\n",
        "    global react_agent\n",
        "\n",
        "    # ========================================================================\n",
        "    # Create the ReAct Agent\n",
        "    # ========================================================================\n",
        "\n",
        "    model = ChatOpenAI(\n",
        "        model=\"gpt-4o\",\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # System message to encourage tool usage\n",
        "    system_message = \"\"\"\n",
        "      You are a learning assistant.\n",
        "\n",
        "      If the user requests:\n",
        "      - summary: summarize the transcript in bullets\n",
        "      - key concepts: extract important ideas\n",
        "      - quiz: generate questions\n",
        "      - grade quiz: evaluate answers using transcript context\n",
        "\n",
        "      If transcript is missing, call get_youtube_transcript first.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the ReAct agent using the built-in function\n",
        "    # This agent handles the thought/action/observation loop internally\n",
        "    react_agent = create_react_agent(\n",
        "        model=model,\n",
        "        tools=tools,\n",
        "        prompt=system_message\n",
        "    )\n",
        "\n",
        "    print(\"[SYSTEM] ReAct agent created successfully\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # Create the Conversation Wrapper Graph\n",
        "    # ========================================================================\n",
        "\n",
        "    workflow = StateGraph(ConversationState)\n",
        "\n",
        "    # Add all nodes\n",
        "    workflow.add_node(\"input\", input_node)\n",
        "    workflow.add_node(\"call_react_agent\", call_react_agent)\n",
        "    workflow.add_node(\"output\", output_node)\n",
        "    workflow.add_node(\"trim_history\", trim_history)\n",
        "    workflow.add_node(\"store_transcript\", store_transcript_node)\n",
        "\n",
        "    # Set entry point - conversation always starts at input\n",
        "    workflow.set_entry_point(\"input\")\n",
        "\n",
        "    # Add conditional edge from input based on command field\n",
        "    workflow.add_conditional_edges(\n",
        "        \"input\",\n",
        "        route_after_input,\n",
        "        {\n",
        "            \"call_react_agent\": \"call_react_agent\",\n",
        "            \"input\": \"input\",  # Loop back for verbose/quiet\n",
        "            \"end\": END\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Add linear edges for the main conversation flow\n",
        "    # Agent -> Output -> Trim -> Input (loops back!)\n",
        "    workflow.add_edge(\"call_react_agent\", \"store_transcript\")\n",
        "    workflow.add_edge(\"store_transcript\", \"output\")\n",
        "    workflow.add_edge(\"output\", \"trim_history\")\n",
        "    workflow.add_edge(\"trim_history\", \"input\")  # This creates the loop!\n",
        "\n",
        "    # Compile the graph\n",
        "    return workflow.compile()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def visualize_graphs(wrapper_app):\n",
        "    \"\"\"\n",
        "    Generate Mermaid diagrams for both graphs.\n",
        "\n",
        "    Creates:\n",
        "    - langchain_react_agent.png: Internal ReAct agent (thought/action/observation)\n",
        "    - langchain_conversation_graph.png: Conversation loop wrapper\n",
        "\n",
        "    Args:\n",
        "        wrapper_app: Compiled conversation graph\n",
        "    \"\"\"\n",
        "    global react_agent\n",
        "\n",
        "    # Visualize the ReAct agent\n",
        "    try:\n",
        "        react_png = react_agent.get_graph().draw_mermaid_png()\n",
        "        with open(\"langchain_react_agent.png\", \"wb\") as f:\n",
        "            f.write(react_png)\n",
        "        print(\"[SYSTEM] ReAct agent graph saved to 'langchain_react_agent.png'\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARNING] Could not generate ReAct agent visualization: {e}\")\n",
        "\n",
        "    # Visualize the conversation wrapper\n",
        "    try:\n",
        "        wrapper_png = wrapper_app.get_graph().draw_mermaid_png()\n",
        "        with open(\"langchain_conversation_graph.png\", \"wb\") as f:\n",
        "            f.write(wrapper_png)\n",
        "        print(\"[SYSTEM] Conversation graph saved to 'langchain_conversation_graph.png'\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARNING] Could not generate conversation graph visualization: {e}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "async def main():\n",
        "    \"\"\"\n",
        "    Main execution function.\n",
        "\n",
        "    This function:\n",
        "    1. Creates the conversation graph\n",
        "    2. Visualizes the graph structure\n",
        "    3. Initializes the conversation state\n",
        "    4. Invokes the graph ONCE\n",
        "\n",
        "    The graph then runs indefinitely via internal looping (trim_history -> input)\n",
        "    until the user types 'quit' or 'exit'.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"LangGraph ReAct Agent - Persistent Multi-Turn Conversation\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nThis system uses create_react_agent with graph-based looping:\")\n",
        "    print(\"  - Single persistent conversation across all turns\")\n",
        "    print(\"  - History managed automatically (trimmed after 100 messages)\")\n",
        "    print(\"  - Loops via graph edges (no Python loops or checkpointing)\")\n",
        "    print(\"\\nCommands:\")\n",
        "    print(\"  - Type 'quit' or 'exit' to end the conversation\")\n",
        "    print(\"  - Type 'verbose' to enable detailed tracing\")\n",
        "    print(\"  - Type 'quiet' to disable detailed tracing\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create the conversation graph\n",
        "    app = create_conversation_graph()\n",
        "\n",
        "    # Visualize both graphs\n",
        "    visualize_graphs(app)\n",
        "\n",
        "    # Initialize conversation state\n",
        "    # This state persists across all turns via graph looping\n",
        "    initial_state = {\n",
        "        \"messages\": [],\n",
        "        \"verbose\": True,\n",
        "        \"command\": None\n",
        "    }\n",
        "\n",
        "    print(\"\\n[SYSTEM] Starting conversation...\\n\")\n",
        "\n",
        "    try:\n",
        "        # Invoke the graph ONCE\n",
        "        # The graph will loop internally until user exits\n",
        "        # Each iteration: input -> agent -> output -> trim -> input (loop!)\n",
        "        # Verbose commands: input -> input (direct loop!)\n",
        "        await app.ainvoke(initial_state)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\n[SYSTEM] Interrupted by user (Ctrl+C)\")\n",
        "\n",
        "    print(\"\\n[SYSTEM] Conversation ended. Goodbye!\\n\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ENTRY POINT\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        asyncio.run(main())\n",
        "    except RuntimeError:\n",
        "        # Running inside Jupyter\n",
        "        import nest_asyncio\n",
        "        nest_asyncio.apply()\n",
        "        asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Iu75Prw--HM",
        "outputId": "7debb5d4-2df6-4c34-fbb4-613731a4321e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "LangGraph ReAct Agent - Persistent Multi-Turn Conversation\n",
            "================================================================================\n",
            "\n",
            "This system uses create_react_agent with graph-based looping:\n",
            "  - Single persistent conversation across all turns\n",
            "  - History managed automatically (trimmed after 100 messages)\n",
            "  - Loops via graph edges (no Python loops or checkpointing)\n",
            "\n",
            "Commands:\n",
            "  - Type 'quit' or 'exit' to end the conversation\n",
            "  - Type 'verbose' to enable detailed tracing\n",
            "  - Type 'quiet' to disable detailed tracing\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2662361698.py:289: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  react_agent = create_react_agent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SYSTEM] ReAct agent created successfully\n",
            "[SYSTEM] ReAct agent graph saved to 'langchain_react_agent.png'\n",
            "[SYSTEM] Conversation graph saved to 'langchain_conversation_graph.png'\n",
            "\n",
            "[SYSTEM] Starting conversation...\n",
            "\n",
            "\n",
            "================================================================================\n",
            "NODE: input_node\n",
            "================================================================================\n",
            "\n",
            "You: can you get the transcript for video 1bUy-1hGZpI\n",
            "[DEBUG] User input: can you get the transcript for video 1bUy-1hGZpI\n",
            "[DEBUG] Routing to call_react_agent\n",
            "\n",
            "================================================================================\n",
            "NODE: output_node\n",
            "================================================================================\n",
            "\n",
            "Assistant: Here is the transcript for the video \"1bUy-1hGZpI\":\n",
            "\n",
            "- There are many large language models available today, each with their own capabilities and specialities.\n",
            "- Lang chain is an open-source orchestration framework designed to work with large language models, available in both Python and JavaScript.\n",
            "- It allows developers to build applications that integrate large language models with data sources and software workflows.\n",
            "- Lang chain uses abstractions to streamline the programming of language model applications, minimizing the amount of code needed for complex NLP tasks.\n",
            "- It includes components like the LLM module, prompt templates, chains, indexes, document loaders, vector databases, text splitters, and memory utilities.\n",
            "- Lang chain supports the creation of chatbots, summarization of text, question answering, data augmentation, and virtual agents.\n",
            "- It is open-source and free, with related frameworks like Lang serve and Lang Smith to enhance functionality.\n",
            "- Lang chain simplifies building applications utilizing large language models.\n",
            "\n",
            "If you need a summary, key concepts, quiz questions, or quiz grading, feel free to ask!\n",
            "\n",
            "================================================================================\n",
            "NODE: input_node\n",
            "================================================================================\n",
            "\n",
            "You: can you give me a summary of the transcript\n",
            "[DEBUG] User input: can you give me a summary of the transcript\n",
            "[DEBUG] Routing to call_react_agent\n",
            "\n",
            "================================================================================\n",
            "NODE: output_node\n",
            "================================================================================\n",
            "\n",
            "Assistant: Here's a summary of the transcript:\n",
            "\n",
            "- **Overview**: Lang chain is an open-source framework designed for developing applications that leverage large language models (LLMs).\n",
            "\n",
            "- **Purpose**: It provides a centralized development environment to integrate LLMs with data sources and workflows, allowing different models to handle various parts of an application.\n",
            "\n",
            "- **Components**:\n",
            "  - **Abstractions**: Simplifies programming by using abstractions, similar to how a thermostat simplifies temperature control.\n",
            "  - **Modules**: Includes LLM modules, prompt templates, chains, indexes, document loaders, vector databases, text splitters, and memory features.\n",
            "  - **LLM Use**: Supports both closed and open-source models, requiring an API key for integration.\n",
            "  - **Chains**: Core of the workflow, allowing sequential execution of functions with different models and prompts.\n",
            "  - **Memory**: Offers utilities to add memory to applications, retaining conversation history.\n",
            "  - **Agents**: Use language models for reasoning and action-taking within applications.\n",
            "\n",
            "- **Use Cases**: Applicable in chatbots, summarization, question answering, data augmentation, and creating virtual agents.\n",
            "\n",
            "- **Additional Tools**: Related frameworks include Lang serve for REST APIs and Lang Smith for application monitoring and debugging.\n",
            "\n",
            "- **Conclusion**: Lang chain enhances the development of applications using LLMs by simplifying integration and execution processes.\n",
            "\n",
            "================================================================================\n",
            "NODE: input_node\n",
            "================================================================================\n",
            "\n",
            "You: can you give me the key concepts\n",
            "[DEBUG] User input: can you give me the key concepts\n",
            "[DEBUG] Routing to call_react_agent\n",
            "\n",
            "================================================================================\n",
            "NODE: output_node\n",
            "================================================================================\n",
            "\n",
            "Assistant: Here are the key concepts from the transcript:\n",
            "\n",
            "- **Lang Chain Framework**: An open-source orchestration platform for developing applications with large language models (LLMs), available in Python and JavaScript.\n",
            "\n",
            "- **Abstractions**: Simplifies the programming of language model applications by using abstractions to minimize coding complexity.\n",
            "\n",
            "- **Modular Components**:\n",
            "  - **LLM Module**: Allows integration with various language models using a standard interface.\n",
            "  - **Prompt Templates**: Formalizes prompt creation to guide LLM responses without hardcoding.\n",
            "  - **Chains**: Core workflows that combine LLMs and other components to perform sequential tasks.\n",
            "\n",
            "- **Indexes and Data Integration**:\n",
            "  - **Document Loaders**: Import data from external sources like file storage, web content, and databases.\n",
            "  - **Vector Databases**: Store data points as vector embeddings for efficient retrieval.\n",
            "  - **Text Splitters**: Divide text into meaningful chunks for processing.\n",
            "\n",
            "- **Memory and Agents**:\n",
            "  - **Memory Utilities**: Add memory to applications for retaining conversation history.\n",
            "  - **Agents**: Use LLMs for reasoning and decision-making to autonomously determine and execute actions.\n",
            "\n",
            "- **Applications and Use Cases**:\n",
            "  - **Chatbots**: Integrate with communication channels and provide context-specific responses.\n",
            "  - **Summarization and Question Answering**: Handle complex texts and provide answers using specific knowledge.\n",
            "  - **Data Augmentation**: Generate synthetic data for machine learning.\n",
            "\n",
            "- **Related Tools**: Lang serve for creating REST APIs and Lang Smith for monitoring and debugging applications.\n",
            "\n",
            "- **Open Source**: Lang chain is free to use, encouraging community contributions and development.\n",
            "\n",
            "================================================================================\n",
            "NODE: input_node\n",
            "================================================================================\n",
            "\n",
            "You: can you generate five easy quiz questions\n",
            "[DEBUG] User input: can you generate five easy quiz questions\n",
            "[DEBUG] Routing to call_react_agent\n",
            "\n",
            "================================================================================\n",
            "NODE: output_node\n",
            "================================================================================\n",
            "\n",
            "Assistant: Here are five easy quiz questions based on the transcript:\n",
            "\n",
            "1. What is Lang chain, and in which programming languages is it available?\n",
            "2. How does Lang chain simplify the programming of applications using large language models?\n",
            "3. What is the purpose of the LLM module in Lang chain?\n",
            "4. Name two use cases for Lang chain mentioned in the transcript.\n",
            "5. What are document loaders used for in Lang chain?\n",
            "\n",
            "If you would like answers to these questions or need further assistance, feel free to ask!\n",
            "\n",
            "================================================================================\n",
            "NODE: input_node\n",
            "================================================================================\n",
            "\n",
            "You: can you give me the answers and explain why\n",
            "[DEBUG] User input: can you give me the answers and explain why\n",
            "[DEBUG] Routing to call_react_agent\n",
            "\n",
            "================================================================================\n",
            "NODE: output_node\n",
            "================================================================================\n",
            "\n",
            "Assistant: Here are the answers to the quiz questions along with explanations:\n",
            "\n",
            "1. **What is Lang chain, and in which programming languages is it available?**\n",
            "   - **Answer**: Lang chain is an open-source orchestration framework for developing applications that use large language models. It is available in Python and JavaScript.\n",
            "   - **Explanation**: The transcript describes Lang chain as a framework designed to facilitate the development of applications using LLMs, supporting both Python and JavaScript libraries.\n",
            "\n",
            "2. **How does Lang chain simplify the programming of applications using large language models?**\n",
            "   - **Answer**: Lang chain simplifies programming by using abstractions, which minimize the amount of code required to execute complex NLP tasks.\n",
            "   - **Explanation**: Abstractions in Lang chain allow developers to manage complex processes in a simplified manner, similar to how a thermostat abstracts the complexity of controlling temperature.\n",
            "\n",
            "3. **What is the purpose of the LLM module in Lang chain?**\n",
            "   - **Answer**: The LLM module provides a standard interface for integrating nearly any large language model using an API key.\n",
            "   - **Explanation**: The LLM module facilitates the use of different language models within Lang chain by offering a consistent interface, making it easier to integrate various models.\n",
            "\n",
            "4. **Name two use cases for Lang chain mentioned in the transcript.**\n",
            "   - **Answer**: Two use cases for Lang chain are chatbots and summarization.\n",
            "   - **Explanation**: The transcript lists several applications for Lang chain, including its use in developing chatbots and for summarizing text.\n",
            "\n",
            "5. **What are document loaders used for in Lang chain?**\n",
            "   - **Answer**: Document loaders import data from external sources like file storage services, web content, and databases.\n",
            "   - **Explanation**: Document loaders facilitate the integration of external data into applications, allowing Lang chain to work with a variety of data sources beyond the language model's training data.\n",
            "\n",
            "================================================================================\n",
            "NODE: input_node\n",
            "================================================================================\n",
            "\n",
            "You: exit\n",
            "[DEBUG] Exit command received\n",
            "[DEBUG] Routing to END (exit requested)\n",
            "\n",
            "[SYSTEM] Conversation ended. Goodbye!\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2662361698.py:441: RuntimeWarning: coroutine 'main' was never awaited\n",
            "  asyncio.run(main())\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    }
  ]
}